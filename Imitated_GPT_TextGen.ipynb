{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_UWwoagQF5R"
      },
      "source": [
        "1. Note that the params are very small... , because colab's free accelerators are slower than my laptop's GPU (Feel free to tune them up if you have colab PRO)\n",
        "2. Ideally to best train this model, do 60 epcohs, so takes like ~ 30minuteish\n",
        "3. Why? GPT is robust because of huge weights = more training needed (for instance, GPT-3 was trained with 45TB of text with 175B Params) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrcodYXCp43e"
      },
      "source": [
        "In my folder, vocabs aren't lower cased because I have larger samples to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfnVnEnxNFzw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# tokens for preprocessing\n",
        "PAD_TAG = \"<PAD>\"\n",
        "SOS_TAG = \"<SOS>\"\n",
        "EOS_TAG = \"<EOS>\"\n",
        "UNK_TAG = \"<UNK>\"\n",
        "PAD = 0\n",
        "SOS = 1\n",
        "EOS = 2\n",
        "UNK = 3\n",
        "\n",
        "max_len = 100 # max number of words per input sequence\n",
        "d_model = 768 # number of dimensions to represent each word\n",
        "batch_size = 128 # batch_size\n",
        "vocab_size = 13360 # how many vocabs are in the tokenizer\n",
        "nheads = 8 # number of Multi-Head Attention heads\n",
        "dim_feedforward = 1024 # number of neurons at the end of each Decoder Block\n",
        "decoder_layers = 3 # Number of Decoder blocks\n",
        "lr = 1e-4 # learning rate\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnhWX6tHOjof"
      },
      "source": [
        "Download this file and upload, this will be data: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u12Rr1fRfXl"
      },
      "source": [
        "1. Click on the link\n",
        "2. Right click\n",
        "3. Click Save as\n",
        "4. Save\n",
        "5. on the left hand side, click on folder icon (below -> {x})\n",
        "6. click upload icon under \"Files\"\n",
        "7. Copy it's uploaded path and use that as path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqK2Lnw9tBvY"
      },
      "source": [
        "# Preprocessing (Data Preparation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzKekXEuL9-c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "<PAD> -> 0    padding\n",
        "<SOS> -> 1    start of sentence\n",
        "<EOS> -> 2    end of sentence\n",
        "<UNK> -> 3    unknown token\n",
        "\"\"\"\n",
        "import json\n",
        "\n",
        "class Word2Sequence:\n",
        "    PAD_TAG = \"<PAD>\"\n",
        "    SOS_TAG = \"<SOS>\"\n",
        "    EOS_TAG = \"<EOS>\"\n",
        "    UNK_TAG = \"<UNK>\"\n",
        "\n",
        "    PAD = 0\n",
        "    SOS = 1\n",
        "    EOS = 2\n",
        "    UNK = 3\n",
        "    \n",
        "    special_tokens = [PAD_TAG, SOS_TAG, EOS_TAG, UNK_TAG]\n",
        "        \n",
        "    def __init__(self, custom_dict = None):\n",
        "        self.dict = {\n",
        "            self.PAD_TAG : self.PAD,\n",
        "            self.SOS_TAG : self.SOS,\n",
        "            self.EOS_TAG : self.EOS,\n",
        "            self.UNK_TAG : self.UNK\n",
        "        } if custom_dict == None else custom_dict\n",
        "        \n",
        "        self.count = {}\n",
        "\n",
        "    def fit(self, sentence):\n",
        "        \"\"\"\n",
        "        param: sentence: [word1, word2, word3...]\n",
        "        \"\"\"\n",
        "        for word in sentence:\n",
        "            self.count[word] = self.count.get(word, 0) + 1\n",
        "\n",
        "    def build_vocab(self, min=5, max=None, max_features=None):\n",
        "        \"\"\"\n",
        "        param min:          \n",
        "        param max:          \n",
        "        param max_features: \n",
        "        returns:            \n",
        "        \"\"\"\n",
        "        if min is not None:\n",
        "            self.count = {word: value for word,value in self.count.items() if value > min}\n",
        "        if max is not None:\n",
        "            self.count = {word: value for word,value in self.count.items if value < max}\n",
        "        if max_features is not None:\n",
        "            temp = sorted(self.count.items(), key=lambda x:x[-1], reverse=True)[:max_features]\n",
        "            self.count = dict(temp)\n",
        "\n",
        "        for word in self.count:\n",
        "            if word not in self.special_tokens:\n",
        "                self.dict[word] = len(self.dict)\n",
        "        \n",
        "        self.reverse_dict = dict(zip(self.dict.values(), self.dict.keys()))\n",
        "    \n",
        "    def transform(self, sentence, max_len=None, pad_first=False):\n",
        "        \"\"\"\n",
        "        param sentence: [word1, word2...]\n",
        "        \"\"\"\n",
        "        if max_len is not None: # do padding here \n",
        "            if pad_first == False:\n",
        "                if max_len > len(sentence):\n",
        "                    sentence = sentence + [self.PAD_TAG] * (max_len-len(sentence))\n",
        "                if max_len < len(sentence):\n",
        "                    sentence = sentence[:max_len] # truncation\n",
        "            else:\n",
        "                if max_len > len(sentence):\n",
        "                    sentence = [self.PAD_TAG] * (max_len-len(sentence)) + sentence\n",
        "                if max_len < len(sentence):\n",
        "                    sentence = sentence[-max_len:] # truncation\n",
        "\n",
        "        return [self.dict.get(word, self.UNK) for word in sentence]\n",
        "    \n",
        "    def inverse_transform(self, indices, is_tensor=False):\n",
        "        \"\"\"\n",
        "        param indices: [1, 2, 3, 4, 5...]\n",
        "        \"\"\"\n",
        "        if is_tensor == False:\n",
        "            return [self.reverse_dict.get(idx) for idx in indices]\n",
        "        \n",
        "        else:\n",
        "            \n",
        "            return [self.reverse_dict.get(idx.item()) for idx in indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2biulAoM8CN"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "\n",
        "def txt_to_wordlist(path):\n",
        "    text = open(path).readlines()\n",
        "    seq_dict = {}\n",
        "    temp_sequence = []\n",
        "    count = 0\n",
        "    for i in range(len(text)):\n",
        "        if text[i] != \"\\n\":\n",
        "            temp_sequence += [text[i]]\n",
        "        else:\n",
        "            seq_dict[count] = temp_sequence\n",
        "            count = count + 1\n",
        "            temp_sequence = []\n",
        "    \n",
        "    word_list = []\n",
        "    for idx, (key, value) in enumerate(seq_dict.items()):\n",
        "        tknzed = [WordPunctTokenizer().tokenize(x) + [\"\\n\"] for x in value]\n",
        "        for sent in tknzed:\n",
        "            for word in sent:\n",
        "                word_list.append(word.lower())\n",
        "                \n",
        "    return word_list\n",
        "\n",
        "def to_sequence(wordlist, max_len):\n",
        "    input_sequences = []\n",
        "    \n",
        "    for i in range(0, len(wordlist)-max_len):\n",
        "        input_sequences.append(wordlist[i:i+max_len])\n",
        "        \n",
        "    return input_sequences\n",
        "\n",
        "class Dataset(Dataset):\n",
        "  def __init__(self, sequences, tokenizer, max_len, limit=None):\n",
        "    self.max_len = max_len\n",
        "    \n",
        "    self.sequences = sequences if limit == None else sequences[:limit]\n",
        "    \n",
        "    self.tokenizer = tokenizer\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    x = [\"<SOS>\"] + self.sequences[idx][:-1]\n",
        "    y = self.sequences[idx][0:-1] + [\"<EOS>\"]\n",
        "    \n",
        "    x = self.tokenizer.transform(x, max_len=self.max_len, pad_first=False)\n",
        "    y = self.tokenizer.transform(y, max_len=self.max_len, pad_first=False)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sequences)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    '''\n",
        "    param batch: ([x, y]ï¼Œ [x, y], output of getitem...)\n",
        "    '''\n",
        "    x, y = list(zip(*batch))\n",
        "    return torch.LongTensor(x), torch.LongTensor(y)\n",
        "\n",
        "def get_dataloader(dataset, batch_size, shuffle=True, drop_last=False, collate_fn=collate_fn):\n",
        "    dataloader = DataLoader(dataset=dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=shuffle,\n",
        "                            drop_last=drop_last,\n",
        "                            collate_fn=collate_fn)\n",
        "    return dataloader\n",
        "\n",
        "word_list = txt_to_wordlist(\"input.txt\")\n",
        "tokenizer = Word2Sequence()\n",
        "tokenizer.fit(word_list)\n",
        "tokenizer.build_vocab(min=0, max_features=None)\n",
        "input_sequences = to_sequence(word_list, max_len)\n",
        "dataset = Dataset(input_sequences, tokenizer, max_len, limit=100)\n",
        "dataloader = get_dataloader(dataset, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N6sne4BNLnE",
        "outputId": "89736fa1-91d0-40d8-e548-b4bd48036ef1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11495\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(tokenizer.dict)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-5CX0WUhUHI",
        "outputId": "dec710aa-0af4-42d5-c8de-85ecdfada8da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data size: 100\n"
          ]
        }
      ],
      "source": [
        "print(f\"Data size: {len(dataloader.dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYh36JerO94y"
      },
      "outputs": [],
      "source": [
        "# # NOTE: You may run this to check the data structure\n",
        "# for i, (x,y) in enumerate(dataloader):\n",
        "#     print(x[0])\n",
        "#     print(y[0])\n",
        "#     break\n",
        "# print(\" \".join(tokenizer.inverse_transform(x[1], is_tensor=True)))\n",
        "# print(\" \".join(tokenizer.inverse_transform(y[1], is_tensor=True)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T6Qg255tGW6"
      },
      "source": [
        "# Model Configuration and Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-hNFBD3PF_e"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len, d_model, nhead, dim_feedforward, num_layers):\n",
        "        super().__init__()\n",
        "        \n",
        "        # [11495 (vocab_size), 768 (d_model)]\n",
        "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
        "        \n",
        "        # [100 (max_len), 768 (d_model)]\n",
        "        self.pos_embed = nn.Parameter(torch.randn(max_len, d_model, device=device) / 10)\n",
        "        \n",
        "        # attention (tril) mask\n",
        "        self.attention_mask = torch.triu(\n",
        "            torch.ones(\n",
        "                (max_len, max_len),\n",
        "                dtype = torch.long,\n",
        "                device = device\n",
        "            ), diagonal=1\n",
        "          )\n",
        "        \n",
        "        self.attention_mask = self.attention_mask == 1\n",
        "        \n",
        "        # So that the mask isn't part of backprop, therefore should be constants\n",
        "        self.register_buffer(\"mask\", self.attention_mask)\n",
        "        \n",
        "        # GPT Decoder Block\n",
        "        self.DecoderBlock = Decoder(d_model, nhead, dim_feedforward, num_layers)\n",
        "        \n",
        "        # output feed forward network\n",
        "        self.FINAL_ffn = nn.Linear(in_features = d_model, out_features = vocab_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # [128 (batch_size), 100 (max_len)]\n",
        "        pad_mask_x = get_key_padding_mask(x)\n",
        "\n",
        "        # all shape: [128 (batch_size), 100 (max_len), 768 (d_model)]\n",
        "        word_embedding = self.embed(x)\n",
        "        \n",
        "        # shape: [128 (batch_size), 100 (max_len), 768 (d_model)]\n",
        "        embedded_x = word_embedding + self.pos_embed\n",
        "        \n",
        "        # shape: [128 (batch_size), 100 (max_len), 768 (d_model)]\n",
        "        output = self.DecoderBlock(embedded_x, self.attention_mask, pad_mask_x)\n",
        "        \n",
        "        output = self.FINAL_ffn(output) # [128 (batch_size), 100 (max_len), 11495 (vocab_size)]\n",
        "        \n",
        "        return output\n",
        "        \n",
        "        \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, num_layers, dropout=0.2):\n",
        "        super().__init__()\n",
        "        \n",
        "        # GPT Decoder Layer\n",
        "        decoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model = d_model,\n",
        "            nhead = nhead,\n",
        "            dim_feedforward = dim_feedforward,\n",
        "            dropout = dropout,\n",
        "            activation = 'gelu'\n",
        "        )\n",
        "\n",
        "        norm = nn.LayerNorm(normalized_shape = d_model)\n",
        "\n",
        "        # Decoder\n",
        "        self.Decoder = nn.TransformerEncoder(encoder_layer = decoder_layer,\n",
        "                                             num_layers = num_layers,\n",
        "                                             norm = norm)\n",
        "        \n",
        "    def forward(self, x, attention_mask_x, pad_mask_x):\n",
        "        # Convert to PyTorch input formats\n",
        "        # [128 (batch_size), 100 (max_len), 768 (d_model)] -> [100 (max_len), 128 (batch_size), 768 (d_model)]\n",
        "        x = x.permute(1, 0, 2)\n",
        "        \n",
        "        out = self.Decoder(src=x, mask=attention_mask_x, src_key_padding_mask=pad_mask_x)\n",
        "    \n",
        "        # [100 (max_len), 128 (batch_size), 768 (d_model)] -> [128 (batch_size), 100 (max_len), 768 (d_model)]\n",
        "        out = out.permute(1, 0, 2)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    \n",
        "def get_key_padding_mask(data, pad_token=PAD):\n",
        "    attentio_mask = data==pad_token\n",
        "    return attentio_mask\n",
        "\n",
        "\n",
        "def select_top_k(predictions, current_loc, k=1):\n",
        "    temped_pred = predictions[0, current_loc, :]\n",
        "    predicted_index = random.choice(\n",
        "        temped_pred.sort(descending=True)[1][:k]\n",
        "    ).item()\n",
        "    \n",
        "    return predicted_index\n",
        "\n",
        "\n",
        "def generate(model, tokenizer, x, k=1, temp=0.7): # pay attention to loc that's grabbing the word\n",
        "    target = ['<SOS>'] + WordPunctTokenizer().tokenize(x.lower())\n",
        "    pred_loc = len(target)\n",
        "    target = tokenizer.transform(target, max_len=max_len, pad_first=False)\n",
        "    target = torch.LongTensor(target).unsqueeze(0)\n",
        "    \n",
        "    for i in range(max_len - pred_loc - 1):\n",
        "        target = target.to(device)\n",
        "        out = model(target)\n",
        "        # temperature generation technique\n",
        "        out = out / temp\n",
        "        # top k sampling generation technique\n",
        "        pred = select_top_k(out, pred_loc-1, k=k)\n",
        "        if pred == 2: # If we encountered <EOS>, we repredict since it causes model to opt out early\n",
        "            i -= 1\n",
        "            continue\n",
        "        target[0][pred_loc] = pred\n",
        "        pred_loc += 1\n",
        "    return target\n",
        "\n",
        "\n",
        "def train(model, dataloader, batch_size, device=None, saving_path=None, epoch=1, lr=1e-4):\n",
        "    # Will have optimizer and loss func set in stones accordingly to the paper\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    for epoch in range(epoch):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        # for _, (x, y) in tqdm(enumerate(dataloader), total=len(dataloader.dataset)/batch_size):\n",
        "        for i, (x, y) in enumerate(dataloader):\n",
        "            x = x.to(device) if device != None else x\n",
        "            y = y.to(device) if device != None else x\n",
        "            \n",
        "            # We essentially consider location of predicted index before padding as prediction\n",
        "            # Then insert that prediction to the next up coming padding index as prediction and so on\n",
        "            \n",
        "            optim.zero_grad()\n",
        "\n",
        "            # [batch_size, max_len] -> [batch_size, max_len, vocab_size]\n",
        "            pred_y = model(x)\n",
        "            \n",
        "            # loss_func([shape from batch_size * max_len, vocab_size], [shape from batch_size * max_len])\n",
        "            loss = loss_func(pred_y.reshape(-1, vocab_size), y.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            \n",
        "            optim.step()\n",
        "            \n",
        "            total_train_loss += loss.item()\n",
        "        \n",
        "        avg_train_loss = total_train_loss / len(dataloader)\n",
        "        \n",
        "        print(f\"EPOCH: {epoch}\", f\"AVG.Loss: {avg_train_loss}\")\n",
        "        total_train_loss = 0\n",
        "        \n",
        "        torch.save(model.state_dict(), saving_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7xbHeVFtKCM"
      },
      "source": [
        "# Training!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn3PjWvvPjop",
        "outputId": "215cd667-1a29-45ee-881a-3e4bfed04695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 0 AVG.Loss: 9.472147941589355\n",
            "EPOCH: 1 AVG.Loss: 8.977290153503418\n",
            "EPOCH: 2 AVG.Loss: 8.494415283203125\n",
            "EPOCH: 3 AVG.Loss: 8.020892143249512\n",
            "EPOCH: 4 AVG.Loss: 7.555840492248535\n",
            "EPOCH: 5 AVG.Loss: 7.118276596069336\n",
            "EPOCH: 6 AVG.Loss: 6.707829475402832\n",
            "EPOCH: 7 AVG.Loss: 6.347342014312744\n",
            "EPOCH: 8 AVG.Loss: 6.040239334106445\n",
            "EPOCH: 9 AVG.Loss: 5.767886638641357\n",
            "EPOCH: 10 AVG.Loss: 5.522299766540527\n",
            "EPOCH: 11 AVG.Loss: 5.27824592590332\n",
            "EPOCH: 12 AVG.Loss: 5.046084403991699\n",
            "EPOCH: 13 AVG.Loss: 4.821122646331787\n",
            "EPOCH: 14 AVG.Loss: 4.602878093719482\n",
            "EPOCH: 15 AVG.Loss: 4.391732215881348\n",
            "EPOCH: 16 AVG.Loss: 4.19173002243042\n",
            "EPOCH: 17 AVG.Loss: 3.9957046508789062\n",
            "EPOCH: 18 AVG.Loss: 3.812227249145508\n",
            "EPOCH: 19 AVG.Loss: 3.6375112533569336\n",
            "EPOCH: 20 AVG.Loss: 3.4667346477508545\n",
            "EPOCH: 21 AVG.Loss: 3.3060622215270996\n",
            "EPOCH: 22 AVG.Loss: 3.1447057723999023\n",
            "EPOCH: 23 AVG.Loss: 2.9887478351593018\n",
            "EPOCH: 24 AVG.Loss: 2.841731309890747\n",
            "EPOCH: 25 AVG.Loss: 2.694150447845459\n",
            "EPOCH: 26 AVG.Loss: 2.5540642738342285\n",
            "EPOCH: 27 AVG.Loss: 2.4239377975463867\n",
            "EPOCH: 28 AVG.Loss: 2.2997779846191406\n",
            "EPOCH: 29 AVG.Loss: 2.1764934062957764\n",
            "EPOCH: 30 AVG.Loss: 2.0703184604644775\n",
            "EPOCH: 31 AVG.Loss: 1.963010311126709\n",
            "EPOCH: 32 AVG.Loss: 1.8674824237823486\n",
            "EPOCH: 33 AVG.Loss: 1.7747853994369507\n",
            "EPOCH: 34 AVG.Loss: 1.6879068613052368\n",
            "EPOCH: 35 AVG.Loss: 1.6122797727584839\n",
            "EPOCH: 36 AVG.Loss: 1.5396634340286255\n",
            "EPOCH: 37 AVG.Loss: 1.4715304374694824\n",
            "EPOCH: 38 AVG.Loss: 1.4132533073425293\n",
            "EPOCH: 39 AVG.Loss: 1.3530734777450562\n",
            "EPOCH: 40 AVG.Loss: 1.3012351989746094\n",
            "EPOCH: 41 AVG.Loss: 1.250133991241455\n",
            "EPOCH: 42 AVG.Loss: 1.204269289970398\n",
            "EPOCH: 43 AVG.Loss: 1.160097360610962\n",
            "EPOCH: 44 AVG.Loss: 1.1179563999176025\n",
            "EPOCH: 45 AVG.Loss: 1.081663727760315\n",
            "EPOCH: 46 AVG.Loss: 1.0494232177734375\n",
            "EPOCH: 47 AVG.Loss: 1.0128813982009888\n",
            "EPOCH: 48 AVG.Loss: 0.9791465997695923\n",
            "EPOCH: 49 AVG.Loss: 0.9514643549919128\n",
            "EPOCH: 50 AVG.Loss: 0.9202971458435059\n",
            "EPOCH: 51 AVG.Loss: 0.8941011428833008\n",
            "EPOCH: 52 AVG.Loss: 0.869213879108429\n",
            "EPOCH: 53 AVG.Loss: 0.8412531018257141\n",
            "EPOCH: 54 AVG.Loss: 0.8156450986862183\n",
            "EPOCH: 55 AVG.Loss: 0.7949836254119873\n",
            "EPOCH: 56 AVG.Loss: 0.7713502645492554\n",
            "EPOCH: 57 AVG.Loss: 0.7499399781227112\n",
            "EPOCH: 58 AVG.Loss: 0.7272129654884338\n",
            "EPOCH: 59 AVG.Loss: 0.7076776623725891\n"
          ]
        }
      ],
      "source": [
        "# NOTE: Initialize model and parameters here\n",
        "gpt = GPT(\n",
        "    vocab_size = vocab_size,\n",
        "    max_len = max_len,\n",
        "    d_model = d_model,\n",
        "    nhead = nheads,\n",
        "    dim_feedforward = dim_feedforward,\n",
        "    num_layers = decoder_layers\n",
        ")\n",
        "gpt.to(device)\n",
        "\n",
        "# NOTE: If Transfer Learning, make sure the HyperParameters match\n",
        "# Uncomment line 14 if you saved a model previously and would like to perform Transfer Learning\n",
        "# gpt.load_state_dict(torch.load(\"/content/GPT.pt\", map_location=device))\n",
        "\n",
        "\n",
        "# NOTE: TRAINING!!!\n",
        "train(\n",
        "    gpt, dataloader, batch_size,\n",
        "    device, saving_path=\"GPT.pt\",\n",
        "    epoch=60, lr=lr\n",
        ")\n",
        "\n",
        "# NOTE: Make sure you download the saved model weights if you would like to load it\n",
        "# Should take 20 min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me2ZNxG5xfWf"
      },
      "source": [
        "It's expected for the model to have repetition and illogical phrases due to constraints of small weights and little data. Again, GPTs are robust because they were trained on large datasets and large weights. (for instance, GPT-3 was trained with 45TB of text with 175B Params) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chFJdYFmSBSB",
        "outputId": "90b88772-14cc-4ee1-ae5a-321fca064260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<SOS> romeo : , you know caius . chief \n",
            " first , and you are accounted poor resolved to die \n",
            " all kill we know ' t a accounted away ! \n",
            " first , the patricians speak . resolved rather speak . resolved rather to the patricians we ' ll have corn kill word resolved rather on ' ll \n",
            " we know know ' ll resolved marcius is chief know ' t ; let it be done : away , good . \n",
            " all kill poor citizens . resolved . \n",
            " first <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ]
        }
      ],
      "source": [
        "# NOTE: Test model by generating!\n",
        "header = \"Romeo:\"\n",
        "generated = generate(gpt, tokenizer, header.lower(), k=3, temp=0.7)\n",
        "converted = \" \".join(tokenizer.inverse_transform(generated.cpu()[0], is_tensor=True))\n",
        "print(converted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbussXyKpaPK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Imitated GPT TextGen.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
